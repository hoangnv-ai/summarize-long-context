from ner import *
from semantic_chungking import *
from agent import Agent
from utils import *
import time
import gradio as gr
import json
import os

def process_text(
    text_input,
    summarize_size_input,
    list_entity_input,
    similarity_threshold,
    min_chunk_size,
    max_chunk_size,
    progress=gr.Progress()
):
    """
    X·ª≠ l√Ω vƒÉn b·∫£n: chunking, entity extraction, v√† summarization
    """
    if not text_input or not text_input.strip():
        return "Vui l√≤ng nh·∫≠p ho·∫∑c upload vƒÉn b·∫£n!", None, ""
    
    list_ner = list_entity_input.split(",")
    summarize_size_input = int(summarize_size_input)
    try:
        # Kh·ªüi t·∫°o chunker
        progress(0.1, desc="ƒêang kh·ªüi t·∫°o chunker...")
        chunker = SemanticNewsChunker(
            similarity_threshold=similarity_threshold,
            min_chunk_size=min_chunk_size,
            max_chunk_size=max_chunk_size
        )
        
        # Th·ª±c hi·ªán chunking
        progress(0.2, desc="ƒêang th·ª±c hi·ªán chunking...")
        chunks = chunker.chunk(text_input, verbose=False)
        
        if not chunks:
            return "Kh√¥ng th·ªÉ t·∫°o chunks t·ª´ vƒÉn b·∫£n. Vui l√≤ng th·ª≠ l·∫°i v·ªõi vƒÉn b·∫£n d√†i h∆°n.", None, ""
        
        system_prompt = load_prompt()
        previous_text = ""
        
        # X·ª≠ l√Ω t·ª´ng chunk
        total_chunks = len(chunks)
        results_html = "<div style='max-height: 600px; overflow-y: auto;'>"
        
        for i_chunk, chunk in enumerate(chunks):
            progress_value = 0.2 + (i_chunk / total_chunks) * 0.7
            progress(progress_value, desc=f"ƒêang x·ª≠ l√Ω chunk {i_chunk + 1}/{total_chunks}...")
            
            agent = Agent(system=system_prompt,
                          max_length=summarize_size_input,                    
                          )
            
            # X·ª≠ l√Ω previous_text
            chunk['previous_text'] = previous_text
            previous_text = chunk['text']
            
            # Th·ª±c hi·ªán tr√≠ch xu·∫•t entity
            list_entity_name = get_entity_name(chunk['text'], list_ner)
            chunk["list_entity"] = list_entity_name
            
            # Th·ª±c hi·ªán t√≥m t·∫Øt
            result = agent(chunk)
            chunk["summarize"] = result
            
            # T·∫°o HTML cho k·∫øt qu·∫£
            results_html += f"""
            <div style='border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px;'>
                <h3 style='color: #2563eb;'>Chunk {chunk['chunk_id']}</h3>
                <p><strong>S·ªë t·ª´:</strong> {chunk['word_count']} | <strong>S·ªë c√¢u:</strong> {chunk['sentence_count']}</p>
                <p><strong>VƒÉn b·∫£n:</strong> {chunk['text'][:200]}...</p>
                <p><strong>Entities:</strong> {', '.join([f"{e['text']} ({e['label']})" for e in list_entity_name[:5]])}</p>
                <p><strong>T√≥m t·∫Øt:</strong> {result[:300]}...</p>
            </div>
            """
            
            # Delay ƒë·ªÉ tr√°nh rate limiting
            if i_chunk % 10 == 0:
                time.sleep(10)
            else:
                time.sleep(1)
        
        results_html += "</div>"
        
        # L∆∞u k·∫øt qu·∫£ v√†o file
        output_path = "./output/result.json"
        os.makedirs("./output", exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        progress(1.0, desc="Ho√†n th√†nh!")
        
        summary_text = f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {total_chunks} chunks.\nK·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {output_path}"
        
        return summary_text, chunks, results_html
        
    except Exception as e:
        error_msg = f"L·ªói: {str(e)}"
        return error_msg, None, ""

# list_ner = list_entity_input.split(",")
# T·∫°o Gradio Interface
with gr.Blocks(title="Text Summarization & Entity Extraction", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # üìù Text Summarization & Entity Extraction
        
        ·ª®ng d·ª•ng n√†y s·∫Ω:
        - Chia vƒÉn b·∫£n th√†nh c√°c chunks d·ª±a tr√™n ng·ªØ nghƒ©a
        - Tr√≠ch xu·∫•t entities (t√™n ng∆∞·ªùi, t·ªï ch·ª©c, s·ª± ki·ªán, v.v.)
        - T√≥m t·∫Øt t·ª´ng chunk
        """
    )
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ‚öôÔ∏è C·∫•u h√¨nh")
            
            text_input = gr.Textbox(
                label="VƒÉn b·∫£n ƒë·∫ßu v√†o:",
                placeholder="Nh·∫≠p vƒÉn b·∫£n ho·∫∑c s·ª≠ d·ª•ng file upload b√™n d∆∞·ªõi...",
                lines=10,
                max_lines=20
            )
            
            file_input = gr.File(
                label="Ho·∫∑c upload file text",
                file_types=[".txt"]
            )

            summarize_size_input = gr.Textbox(
                label="ƒê·ªô d√†i t·ªëi ƒëa c·ªßa vƒÉn b·∫£n t√≥m t·∫Øt(ƒë∆°n v·ªã %):", 
                placeholder="ƒê·ªô d√†i t·ªëi ƒëa c·ªßa vƒÉn b·∫£n t√≥m t·∫Øt: b·∫±ng bao nhi√™u ph·∫ßn trƒÉm so v·ªõi ban ƒë·∫ßu. Kho·∫£ng gi√° tr·ªã t·ª´ 10 ƒë·∫øn 100",
                lines=1,
                max_lines=1
            )

            list_entity_input = gr.Textbox(
                label="C√°c entity c·∫ßn tr√≠ch xu·∫•t:",
                placeholder='''Nh·∫≠p c√°c lo·∫°i th√¥ng tin c·∫ßn tr√≠ch xu·∫•t ch√≠nh x√°c trong vƒÉn b·∫£n.NgƒÉn c√°ch nhau b·∫±ng d·∫•y ph·∫©y.V√≠ d·ª•:
t√™n s·ª± ki·ªán, t√™n ng∆∞·ªùi, t√™n t·ªï ch·ª©c, m·ªëc th·ªùi gian, v·ªã tr√≠, ti·ªÅn t·ªá, ph·∫ßn trƒÉm.
                ''',
                lines=5,
                max_lines=10
            )

            similarity_threshold = gr.Slider(
                label="Ng∆∞·ª°ng similarity",
                minimum=0.1,
                maximum=1.0,
                value=0.50,
                step=0.05,
                info="Ng∆∞·ª°ng ƒë·ªÉ t√°ch chunk (0-1)"
            )
            
            min_chunk_size = gr.Slider(
                label="K√≠ch th∆∞·ªõc chunk t·ªëi thi·ªÉu (t·ª´)",
                minimum=50,
                maximum=500,
                value=200,
                step=50
            )
            
            max_chunk_size = gr.Slider(
                label="K√≠ch th∆∞·ªõc chunk t·ªëi ƒëa (t·ª´)",
                minimum=200,
                maximum=1000,
                value=500,
                step=50
            )
            
            process_btn = gr.Button("üöÄ X·ª≠ l√Ω", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            gr.Markdown("### üìä K·∫øt qu·∫£")
            
            summary_output = gr.Textbox(
                label="T√≥m t·∫Øt",
                lines=3,
                interactive=False
            )
            
            results_html = gr.HTML(label="Chi ti·∫øt k·∫øt qu·∫£")
            
            json_output = gr.JSON(
                label="D·ªØ li·ªáu JSON",
                visible=False
            )
            
            download_btn = gr.File(
                label="T·∫£i xu·ªëng k·∫øt qu·∫£ JSON",
                visible=False
            )
    
    # X·ª≠ l√Ω file upload
    def load_file(file):
        if file is None:
            return ""
        try:
            with open(file.name, "r", encoding="utf-8") as f:
                return f.read()
        except (IOError, OSError, UnicodeDecodeError) as e:
            return f"L·ªói khi ƒë·ªçc file: {str(e)}"
    
    file_input.change(fn=load_file, inputs=file_input, outputs=text_input)
    
    # X·ª≠ l√Ω khi nh·∫•n n√∫t
    def process_and_display(text, summarize_size_input, list_entity_input, sim_thresh, min_size, max_size, progress=gr.Progress()):
        summary, json_data, html = process_text(text, summarize_size_input, list_entity_input, sim_thresh, min_size, max_size, progress)
        
        outputs = [summary, html]
        
        if json_data:
            outputs.append(json_data)
            # Return the file path for download
            output_path = os.path.abspath("./output/result.json")
            if os.path.exists(output_path):
                outputs.append(gr.update(visible=True, value=output_path))
            else:
                outputs.append(gr.update(visible=False))
        else:
            outputs.append(None)
            outputs.append(gr.update(visible=False))
        
        return outputs
    
    process_btn.click(
        fn=process_and_display,
        inputs=[text_input, summarize_size_input, list_entity_input, similarity_threshold, min_chunk_size, max_chunk_size],
        outputs=[summary_output, results_html, json_output, download_btn]
    )
    
    gr.Markdown(
        """
        ---
        ### üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        1. Nh·∫≠p vƒÉn b·∫£n v√†o √¥ text ho·∫∑c upload file .txt
        2. ƒêi·ªÅu ch·ªânh c√°c tham s·ªë chunking n·∫øu c·∫ßn
        3. Nh·∫•n n√∫t "X·ª≠ l√Ω" v√† ch·ªù k·∫øt qu·∫£
        4. Xem chi ti·∫øt k·∫øt qu·∫£ v√† t·∫£i xu·ªëng file JSON n·∫øu c·∫ßn
        """
    )

if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)